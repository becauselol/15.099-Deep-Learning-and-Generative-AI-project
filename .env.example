# Example .env file for ML model training
# Copy this to .env and modify as needed

# Data file paths
PROMPTS_FILE=../data_gen/countdown_prompts_gemini_transformed.csv
RESULTS_FILE=../data_gen/countdown_results_gemini_done.csv
DATA_FILE=../data_gen/countdown_results_with_prompt_gemini.csv

# Train/Test split data files (generated by create_train_test_split.py)
TRAIN_DATA_FILE=../data_gen/train_countdown_results_with_prompt_gemini.csv
TEST_DATA_FILE=../data_gen/test_countdown_results_with_prompt_gemini.csv

# Feature configuration for unified models
# Options: text_only, features_only, inst_only, prompt_only, text_prompt, all
FEATURE_MODE=text_only
USE_BERT_EMBEDDINGS=true

# Model configuration
MODEL_NAME=bert-base-uncased
MAX_LENGTH=512
TEST_SIZE=0.2
RANDOM_STATE=42

# BERT training configuration
BERT_BATCH_SIZE=16
BERT_LEARNING_RATE=2e-5
BERT_NUM_EPOCHS=3
BERT_WEIGHT_DECAY=0.01

# XGBoost configuration
XGB_ETA=0.1
XGB_MAX_DEPTH=5
XGB_N_ESTIMATORS=100

# ANN configuration
ANN_HIDDEN_SIZES=256,128,64
ANN_DROPOUT=0.3
ANN_LEARNING_RATE=0.001
ANN_BATCH_SIZE=32
ANN_NUM_EPOCHS=20

# Wandb configuration
WANDB_PROJECT=bert-prompt-correctness

# HuggingFace token (replace with your own token)
HF_TOKEN=your_huggingface_token_here

# Output paths
BERT_MODEL_PATH=./bert_finetuned_model
BERT_RESULTS_DIR=./results
BERT_LOGS_DIR=./logs
BERT_SUMMARY_FILE=./training_summary.json

XGBOOST_MODEL_PATH=./xgboost_model.json
XGBOOST_SUMMARY_FILE=./xgboost_training_summary.json

ANN_MODEL_PATH=./ann_model_best.pth
ANN_SUMMARY_FILE=./ann_training_summary.json
